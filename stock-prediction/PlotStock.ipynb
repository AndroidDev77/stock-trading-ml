{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "import os\n",
    "import h5py    \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "import tensorflow as tf\n",
    "import datetime\n",
    "\n",
    "from tensorflow.python.keras.models import Sequential\n",
    "from tensorflow.python.keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from yahoo_fin import stock_info as si\n",
    "from collections import deque\n",
    "from tensorflow.python.keras.callbacks import ModelCheckpoint, TensorBoard\n",
    "\n",
    "from fastai.tabular import add_datepart\n",
    "\n",
    "\n",
    "# set seed, so we can get the same results after rerunning several times\n",
    "#np.random.seed(1)\n",
    "#tf.random.set_seed(1)\n",
    "#random.seed(1)\n",
    "\n",
    "def load_data(ticker, date_now, n_steps=50, scale=True, shuffle=False, lookup_step=1,\n",
    "                test_size=0.2, feature_columns=['adjclose', 'volume', 'open', 'high', 'low', 'date']):\n",
    "    \"\"\"\n",
    "    Loads data from Yahoo Finance source, as well as scaling, shuffling, normalizing and splitting.\n",
    "    Params:\n",
    "        ticker (str/pd.DataFrame): the ticker you want to load, examples include AAPL, TESL, etc.\n",
    "        date_now (str): end date for data\n",
    "        n_steps (int): the historical sequence length (i.e window size) used to predict, default is 50\n",
    "        scale (bool): whether to scale prices from 0 to 1, default is True\n",
    "        shuffle (bool): whether to shuffle the data, default is True\n",
    "        lookup_step (int): the future lookup step to predict, default is 1 (e.g next day)\n",
    "        test_size (float): ratio for test data, default is 0.2 (20% testing data)\n",
    "        feature_columns (list): the list of features to use to feed into the model, default is everything grabbed from yahoo_fin\n",
    "    \"\"\"\n",
    "    # see if ticker is already a loaded stock from yahoo finance\n",
    "    if isinstance(ticker, str):\n",
    "        # load it from yahoo_fin library\n",
    "        enddatestr = datetime.datetime.strptime(date_now, \"%Y-%m-%d\").strftime(\"%m/%d/%Y\")\n",
    "        df = si.get_data(ticker,start_date = '01/01/1994', end_date = enddatestr)\n",
    "        #spydf = si.get_data('SPY', start_date = '01/01/2019', end_date = enddatestr, index_as_date = False)\n",
    "        #df['SPY'] = spydf['adjclose']\n",
    "        add_datepart(df, 'date')\n",
    "        #df.index = df.Elapsed\n",
    "        #df.drop(['Elapsed', 'Dayofyear', 'Week', 'Year', 'Day',\n",
    "        #         'Is_year_start', 'Is_year_end', 'Is_month_end',\n",
    "        #         'Is_month_start'], axis='columns', inplace=True)\n",
    "    elif isinstance(ticker, pd.DataFrame):\n",
    "        # already loaded, use it directly\n",
    "        print(\"Using previously loaded data\")\n",
    "        df = ticker\n",
    "    else:\n",
    "        raise TypeError(\"ticker can be either a str or a `pd.DataFrame` instances\")\n",
    "\n",
    "    # this will contain all the elements we want to return from this function\n",
    "    result = {}\n",
    "    # we will also return the original dataframe itself\n",
    "    result['df'] = df.copy()\n",
    "\n",
    "    # make sure that the passed feature_columns exist in the dataframe\n",
    "    for col in feature_columns:\n",
    "        assert col in df.columns\n",
    "\n",
    "    if scale:\n",
    "        column_scaler = {}\n",
    "        # scale the data (prices) from 0 to 1\n",
    "        for column in feature_columns:\n",
    "            scaler = preprocessing.MinMaxScaler()\n",
    "            df[column] = scaler.fit_transform(np.expand_dims(df[column].values, axis=1))\n",
    "            column_scaler[column] = scaler\n",
    "\n",
    "        # add the MinMaxScaler instances to the result returned\n",
    "        result[\"column_scaler\"] = column_scaler\n",
    "\n",
    "    # add the target column (label) by shifting by `lookup_step`\n",
    "    df['future'] = df['adjclose'].shift(-lookup_step)\n",
    "\n",
    "    # last `lookup_step` columns contains NaN in future column\n",
    "    # get them before droping NaNs\n",
    "    last_sequence = np.array(df[feature_columns].tail(lookup_step))\n",
    "    # drop NaNs\n",
    "    df.dropna(inplace=True)\n",
    "\n",
    "    sequence_data = []\n",
    "    sequences = deque(maxlen=n_steps)\n",
    "\n",
    "    for entry, target in zip(df[feature_columns].values, df['future'].values):\n",
    "        sequences.append(entry)\n",
    "        if len(sequences) == n_steps:\n",
    "            sequence_data.append([np.array(sequences), target])\n",
    "\n",
    "    # get the last sequence by appending the last `n_step` sequence with `lookup_step` sequence\n",
    "    # for instance, if n_steps=50 and lookup_step=10, last_sequence should be of 59 (that is 50+10-1) length\n",
    "    # this last_sequence will be used to predict in future dates that are not available in the dataset\n",
    "    last_sequence = list(sequences) + list(last_sequence)\n",
    "    # shift the last sequence by -1\n",
    "    last_sequence = np.array(pd.DataFrame(last_sequence).shift(-1).dropna())\n",
    "    # add to result\n",
    "    result['last_sequence'] = last_sequence\n",
    "    # construct the X's and y's\n",
    "    X, y = [], []\n",
    "    for seq, target in sequence_data:\n",
    "        X.append(seq)\n",
    "        y.append(target)\n",
    "\n",
    "    # convert to numpy arrays\n",
    "    X = np.array(X)\n",
    "    y = np.array(y)\n",
    "\n",
    "    # reshape X to fit the neural network\n",
    "    X = X.reshape((X.shape[0], X.shape[2], X.shape[1]))\n",
    "    # split the dataset\n",
    "    result[\"X_train\"], result[\"X_test\"], result[\"y_train\"], result[\"y_test\"] = train_test_split(X, y, test_size=test_size, shuffle=shuffle)\n",
    "    # return the result\n",
    "    return result\n",
    "\n",
    "\n",
    "def create_model(sequence_length, units=256, cell=LSTM, n_layers=2, dropout=0.3,\n",
    "                loss=\"mean_absolute_error\", optimizer=\"rmsprop\", bidirectional=False):\n",
    "    model = Sequential()\n",
    "    for i in range(n_layers):\n",
    "        if i == 0:\n",
    "            # first layer\n",
    "            if bidirectional:\n",
    "                model.add(Bidirectional(cell(units, return_sequences=True), input_shape=(None, sequence_length)))\n",
    "            else:\n",
    "                model.add(cell(units, return_sequences=True, input_shape=(None, sequence_length)))\n",
    "        elif i == n_layers - 1:\n",
    "            # last layer\n",
    "            if bidirectional:\n",
    "                model.add(Bidirectional(cell(units, return_sequences=False)))\n",
    "            else:\n",
    "                model.add(cell(units, return_sequences=False))\n",
    "        else:\n",
    "            # hidden layers\n",
    "            if bidirectional:\n",
    "                model.add(Bidirectional(cell(units, return_sequences=True)))\n",
    "            else:\n",
    "                model.add(cell(units, return_sequences=True))\n",
    "        # add dropout after each layer\n",
    "        model.add(Dropout(dropout))\n",
    "    model.add(Dense(1, activation=\"linear\"))\n",
    "    model.compile(loss=loss, metrics=[\"mean_absolute_error\"], optimizer=optimizer)\n",
    "    print(\"Finished Creating Model\")\n",
    "    return model\n",
    "\n",
    "def plot_graph(model, data):\n",
    "    y_test = data[\"y_test\"]\n",
    "    X_test = data[\"X_test\"]\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_test = np.squeeze(data[\"column_scaler\"][\"adjclose\"].inverse_transform(np.expand_dims(y_test, axis=0)))\n",
    "    y_pred = np.squeeze(data[\"column_scaler\"][\"adjclose\"].inverse_transform(y_pred))\n",
    "    plt.plot(y_test[-200:], c='b')\n",
    "    plt.plot(y_pred[-200:], c='r')\n",
    "    plt.xlabel(\"Days\")\n",
    "    plt.ylabel(\"Price\")\n",
    "    plt.legend([\"Actual Price\", \"Predicted Price\"])\n",
    "    plt.show()\n",
    "\n",
    "def get_accuracy(model, data):\n",
    "    y_test = data[\"y_test\"]\n",
    "    X_test = data[\"X_test\"]\n",
    "    y_pred = model.predict(X_test)\n",
    "    y_test = np.squeeze(data[\"column_scaler\"][\"adjclose\"].inverse_transform(np.expand_dims(y_test, axis=0)))\n",
    "    y_pred = np.squeeze(data[\"column_scaler\"][\"adjclose\"].inverse_transform(y_pred))\n",
    "    y_pred = list(map(lambda current, future: int(float(future) > float(current)), y_test[:-LOOKUP_STEP], y_pred[LOOKUP_STEP:]))\n",
    "    y_test = list(map(lambda current, future: int(float(future) > float(current)), y_test[:-LOOKUP_STEP], y_test[LOOKUP_STEP:]))\n",
    "    return accuracy_score(y_test, y_pred)\n",
    "\n",
    "def predict(model, data, classification=False):\n",
    "    # retrieve the last sequence from data\n",
    "    last_sequence = data[\"last_sequence\"][:N_STEPS]\n",
    "    # retrieve the column scalers\n",
    "    column_scaler = data[\"column_scaler\"]\n",
    "    # reshape the last sequence\n",
    "    last_sequence = last_sequence.reshape((last_sequence.shape[1], last_sequence.shape[0]))\n",
    "    # expand dimension\n",
    "    last_sequence = np.expand_dims(last_sequence, axis=0)\n",
    "    # get the prediction (scaled from 0 to 1)\n",
    "    prediction = model.predict(last_sequence)\n",
    "    # get the price (by inverting the scaling)\n",
    "    predicted_price = column_scaler[\"adjclose\"].inverse_transform(prediction)[0][0]\n",
    "    return predicted_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Window size or the sequence length\n",
    "N_STEPS = 10\n",
    "# Lookup step, 1 is the next day\n",
    "LOOKUP_STEP = 1\n",
    "\n",
    "# test ratio size, 0.2 is 20%\n",
    "TEST_SIZE = 0.2\n",
    "# features to use\n",
    "FEATURE_COLUMNS = [\"adjclose\", \"volume\", \"open\", \"high\", \"low\"]\n",
    "# date now\n",
    "date_now = time.strftime(\"%Y-%m-%d\")\n",
    "#date_now = \"2020-06-29\"\n",
    "### model parameters\n",
    "\n",
    "N_LAYERS = 3\n",
    "# LSTM cell\n",
    "CELL = LSTM\n",
    "# 256 LSTM neurons\n",
    "UNITS = 256\n",
    "# 40% dropout\n",
    "DROPOUT = 0.4\n",
    "# whether to use bidirectional RNNs\n",
    "BIDIRECTIONAL = True\n",
    "\n",
    "### training parameters\n",
    "\n",
    "# mean squared error loss\n",
    "LOSS = \"huber_loss\"\n",
    "OPTIMIZER = \"adam\"\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 300\n",
    "\n",
    "# Apple stock market\n",
    "ticker = \"TSLA\"\n",
    "ticker_data_filename = os.path.join(\"data\", f\"{ticker}_{date_now}.csv\")\n",
    "# model name to save\n",
    "model_name = f\"{date_now}_{ticker}-{LOSS}-{CELL.__name__}-seq-{N_STEPS}-step-{LOOKUP_STEP}-layers-{N_LAYERS}-units-{UNITS}\"\n",
    "\n",
    "if BIDIRECTIONAL:\n",
    "    model_name += \"-b\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create these folders if they does not exist\n",
    "if not os.path.isdir(\"results\"):\n",
    "    os.mkdir(\"results\")\n",
    "\n",
    "if not os.path.isdir(\"logs\"):\n",
    "    os.mkdir(\"logs\")\n",
    "\n",
    "if not os.path.isdir(\"data\"):\n",
    "    os.mkdir(\"data\")\n",
    "    \n",
    "#if os.path.exists(ticker_data_filename):\n",
    "#    ticker = pd.read_csv(ticker_data_filename)\n",
    "    \n",
    "# load the data\n",
    "data = load_data(ticker, N_STEPS, lookup_step=LOOKUP_STEP, test_size=TEST_SIZE, feature_columns=FEATURE_COLUMNS)\n",
    "\n",
    "# save the dataframe\n",
    "data[\"df\"].to_csv(ticker_data_filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "{'chart': {'result': None, 'error': {'code': 'Not Found', 'description': 'No data found, symbol may be delisted'}}}",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-1c91498441e4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0menddatestr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrptime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdate_now\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"%Y-%m-%d\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstrftime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"%m/%d/%Y\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#df = si.get_data(ticker,start_date = '01/01/1994', end_date = enddatestr)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mdatedf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msi\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'ticker'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstart_date\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'01/01/2019'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mend_date\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menddatestr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindex_as_date\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;31m#df['SPY'] = spydf['adjclose']\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0madd_datepart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdatedf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'date'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/lib/python3.7/site-packages/yahoo_fin/stock_info.py\u001b[0m in \u001b[0;36mget_data\u001b[0;34m(ticker, start_date, end_date, index_as_date, interval)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mok\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0mAssertionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjson\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAssertionError\u001b[0m: {'chart': {'result': None, 'error': {'code': 'Not Found', 'description': 'No data found, symbol may be delisted'}}}"
     ]
    }
   ],
   "source": [
    "from fastai.tabular import add_datepart\n",
    "date_now = '2020-08-22'\n",
    "enddatestr = datetime.datetime.strptime(date_now, \"%Y-%m-%d\").strftime(\"%m/%d/%Y\")\n",
    "#df = si.get_data(ticker,start_date = '01/01/1994', end_date = enddatestr)\n",
    "datedf = si.get_data(ticker, start_date = '01/01/2019', end_date = enddatestr, index_as_date = False)\n",
    "#df['SPY'] = spydf['adjclose']\n",
    "add_datepart(datedf, 'date')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>open</th>\n",
       "      <th>high</th>\n",
       "      <th>low</th>\n",
       "      <th>close</th>\n",
       "      <th>adjclose</th>\n",
       "      <th>volume</th>\n",
       "      <th>ticker</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2010-06-29</th>\n",
       "      <td>19.000000</td>\n",
       "      <td>25.000000</td>\n",
       "      <td>17.540001</td>\n",
       "      <td>23.889999</td>\n",
       "      <td>23.889999</td>\n",
       "      <td>18766300</td>\n",
       "      <td>TSLA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-06-30</th>\n",
       "      <td>25.790001</td>\n",
       "      <td>30.420000</td>\n",
       "      <td>23.299999</td>\n",
       "      <td>23.830000</td>\n",
       "      <td>23.830000</td>\n",
       "      <td>17187100</td>\n",
       "      <td>TSLA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-07-01</th>\n",
       "      <td>25.000000</td>\n",
       "      <td>25.920000</td>\n",
       "      <td>20.270000</td>\n",
       "      <td>21.959999</td>\n",
       "      <td>21.959999</td>\n",
       "      <td>8218800</td>\n",
       "      <td>TSLA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-07-02</th>\n",
       "      <td>23.000000</td>\n",
       "      <td>23.100000</td>\n",
       "      <td>18.709999</td>\n",
       "      <td>19.200001</td>\n",
       "      <td>19.200001</td>\n",
       "      <td>5139800</td>\n",
       "      <td>TSLA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010-07-06</th>\n",
       "      <td>20.000000</td>\n",
       "      <td>20.000000</td>\n",
       "      <td>15.830000</td>\n",
       "      <td>16.110001</td>\n",
       "      <td>16.110001</td>\n",
       "      <td>6866900</td>\n",
       "      <td>TSLA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-17</th>\n",
       "      <td>1677.000000</td>\n",
       "      <td>1845.859985</td>\n",
       "      <td>1672.829956</td>\n",
       "      <td>1835.640015</td>\n",
       "      <td>1835.640015</td>\n",
       "      <td>20242300</td>\n",
       "      <td>TSLA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-18</th>\n",
       "      <td>1898.989990</td>\n",
       "      <td>1923.900024</td>\n",
       "      <td>1845.109985</td>\n",
       "      <td>1887.089966</td>\n",
       "      <td>1887.089966</td>\n",
       "      <td>16474500</td>\n",
       "      <td>TSLA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-19</th>\n",
       "      <td>1865.000000</td>\n",
       "      <td>1911.000000</td>\n",
       "      <td>1841.209961</td>\n",
       "      <td>1878.530029</td>\n",
       "      <td>1878.530029</td>\n",
       "      <td>12205300</td>\n",
       "      <td>TSLA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-20</th>\n",
       "      <td>1860.680054</td>\n",
       "      <td>2021.989990</td>\n",
       "      <td>1857.060059</td>\n",
       "      <td>2001.829956</td>\n",
       "      <td>2001.829956</td>\n",
       "      <td>20611800</td>\n",
       "      <td>TSLA</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2020-08-21</th>\n",
       "      <td>2044.760010</td>\n",
       "      <td>2095.489990</td>\n",
       "      <td>2025.050049</td>\n",
       "      <td>2049.979980</td>\n",
       "      <td>2049.979980</td>\n",
       "      <td>21428800</td>\n",
       "      <td>TSLA</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2556 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                   open         high          low        close     adjclose  \\\n",
       "2010-06-29    19.000000    25.000000    17.540001    23.889999    23.889999   \n",
       "2010-06-30    25.790001    30.420000    23.299999    23.830000    23.830000   \n",
       "2010-07-01    25.000000    25.920000    20.270000    21.959999    21.959999   \n",
       "2010-07-02    23.000000    23.100000    18.709999    19.200001    19.200001   \n",
       "2010-07-06    20.000000    20.000000    15.830000    16.110001    16.110001   \n",
       "...                 ...          ...          ...          ...          ...   \n",
       "2020-08-17  1677.000000  1845.859985  1672.829956  1835.640015  1835.640015   \n",
       "2020-08-18  1898.989990  1923.900024  1845.109985  1887.089966  1887.089966   \n",
       "2020-08-19  1865.000000  1911.000000  1841.209961  1878.530029  1878.530029   \n",
       "2020-08-20  1860.680054  2021.989990  1857.060059  2001.829956  2001.829956   \n",
       "2020-08-21  2044.760010  2095.489990  2025.050049  2049.979980  2049.979980   \n",
       "\n",
       "              volume ticker  \n",
       "2010-06-29  18766300   TSLA  \n",
       "2010-06-30  17187100   TSLA  \n",
       "2010-07-01   8218800   TSLA  \n",
       "2010-07-02   5139800   TSLA  \n",
       "2010-07-06   6866900   TSLA  \n",
       "...              ...    ...  \n",
       "2020-08-17  20242300   TSLA  \n",
       "2020-08-18  16474500   TSLA  \n",
       "2020-08-19  12205300   TSLA  \n",
       "2020-08-20  20611800   TSLA  \n",
       "2020-08-21  21428800   TSLA  \n",
       "\n",
       "[2556 rows x 7 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# construct the model\n",
    "model = create_model(N_STEPS, loss=LOSS, units=UNITS, cell=CELL, n_layers=N_LAYERS,\n",
    "                    dropout=DROPOUT, optimizer=OPTIMIZER, bidirectional=BIDIRECTIONAL)\n",
    "\n",
    "# some tensorflow callbacks\n",
    "checkpointer = ModelCheckpoint(os.path.join(\"results\", model_name), save_weights_only=True, save_best_only=True, verbose=1)\n",
    "tensorboard = TensorBoard(log_dir=os.path.join(\"logs\", model_name))\n",
    "\n",
    "history = model.fit(data[\"X_train\"], data[\"y_train\"],\n",
    "                    batch_size=BATCH_SIZE,\n",
    "                    epochs=EPOCHS,\n",
    "                    validation_data=(data[\"X_test\"], data[\"y_test\"]),\n",
    "                    callbacks=[checkpointer, tensorboard],\n",
    "                    verbose=1)\n",
    "\n",
    "model.save(os.path.join(\"results\", model_name) + \".h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the data\n",
    "data = load_data(ticker, N_STEPS, lookup_step=LOOKUP_STEP, test_size=TEST_SIZE,\n",
    "                feature_columns=FEATURE_COLUMNS, shuffle=False)\n",
    "\n",
    "# construct the model\n",
    "model = create_model(N_STEPS, loss=LOSS, units=UNITS, cell=CELL, n_layers=N_LAYERS,\n",
    "                    dropout=DROPOUT, optimizer=OPTIMIZER)\n",
    "print(os.getcwd())\n",
    "model_path = os.path.join(\"results\", model_name) + \".h5\"\n",
    "print(model_path)\n",
    "print(\"File Exists:\" + str(os.path.exists(model_path)))\n",
    "\n",
    "model.load_weights(model_path)\n",
    "\n",
    "# evaluate the model\n",
    "mse, mae = model.evaluate(data[\"X_test\"], data[\"y_test\"])\n",
    "# calculate the mean absolute error (inverse scaling)\n",
    "mean_absolute_error = data[\"column_scaler\"][\"adjclose\"].inverse_transform(mae.reshape(1, -1))[0][0]\n",
    "print(\"Mean Absolute Error:\", mean_absolute_error)\n",
    "# predict the future price\n",
    "future_price = predict(model, data)\n",
    "print(f\"Future price after {LOOKUP_STEP} days is {future_price:.2f}$\")\n",
    "print(\"Accuracy Score:\", get_accuracy(model, data))\n",
    "plot_graph(model, data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
